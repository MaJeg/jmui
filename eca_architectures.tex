% presentation SAIBA and ASAP
% related work

Efforts have been made in the last years to create architectures that permit to integrate different components of an ECA in a modular fashion. SAIBA is an architecture that specifically specify a standard for the output generation of embodied conversational agents. It divides the generation of the agent's behavior in three stages, the intent planner generates the communicative intentions of the agent that are later transcribed into multimodal communicative actions by the behavior planner that then send those actions into the behavior realizers. SAIBA specified two XML-based languages to specify how communication is realized between the different modules, the Function Markup Language for the communication between the Intent Planner and the Behavior Planner and the Behavior Markup Language for the communication between the Behavior Planner and the Behavior Realizer. 
Virtual Human Toolkit extends the three stages generation principles of SAIBA with the specification of the input interpretation. More specifically, Virtual Human Toolkit divide the input processing in an audio-visual sensing module, a non-verbal behavior understanding module, a speech recognition module and a natural language understanding module. These different modules communicate by using the perception markup language. The output generation step follow the three stages process of SAIBA, with modules communicating with the FML and BML languages but divide the behavior planning step into nonverbal generation and natural language generation and the realization step into Speech Generation, Behavior Generation and Rendering. 

These two architectures are examples of pipeline and event-based architectures, once a communicative intention is generated by the agent, it is transcribed into multimodal actions and then launched by the realizer without possibility to modulate or interrupt the action. In the same manner, in the modules dedicated to the generation of communicative intentions, the decision to generate intentions is based on events detected by the perceptors. 

On the contrary, our model perceives and controls the actions of the agent in a continuous fashion: actions of the agent are continuously modulated rather than triggered by decision modules, and the perception and decision processes run concurrently rather than sequentially. 
These requirements follows the design principle of the recent architecture ASAP. This architecture is an extension of the SAIBA architecture \cite{kopp_architecture_2014} designed to add incremental interpretation and generation capabilities. This architecture follows the three stage output generation architecture of SAIBA, and specify also three different modules for the input processing of the user's behavior, the Sensing, Behavior Interpretation and Function Interpretation modules. 
In order to be able to interrupt or modulate actions that have already been launched by the behavior realizer, \cite{kopp_architecture_2014} proposes an extension of the BML language, the BMLa.  
An other difference with the SAIBA framework resides in the fact that the Behavior Planner module is not only a simple rule-based module aimed at transcribing the communicative intentions generated by the intent planner into multimodal commuicative actions, but it also receives, as an input, a continuous flow of low-level perception data provided by the behavior interpretation modules that permit to rapidly modulate or interrupt the actions already launched. This rapid perception-action loop permit for example to the agent to rapidly react to user interruptions, either by raising his voice or by interrupting itself.   
