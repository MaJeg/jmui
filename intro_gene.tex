\section{Introduction}

Natural spoken dialog systems are nowadays used daily by millions of users with the recent development of virtual assistants like SIRI, Google Now or Cortana.
These systems have nowadays very efficient and reliable speech recognition and synthesis capabilities but have only very limited abilities to handle other aspects of user-agent multi-modal interactions.
Moreover, they are designed to support question/answering scenarios but not mixed-initiative dialogs where both the system or the user may take the initiative to speak, eventually concurrently. One of their missing capabilities is their ability to coordinate speaking turns with the users. Existing systems merely use simple rules: never speak at the same time the user is speaking, when the user barges in, stop speaking to let the user speak, and take the turn as soon as possible after the user's end of turn. Real conversations are more complex: speech overlaps are not so rare and conflictual situations, were both participants wants to have the floor at the same time are also common. Moreover the duration of silences during transitions between two turns are highly variable. 
Improving the naturalness of user-agent interactions requires to take into account these characteristic of human conversations.

%One could expect that the next generations of these assistants become fully embodied conversational agents, having the ability to interact verbally and non verbally with users. 
%These systems have nowadays very efficient and reliable speech recognition and synthesis capabilities but have only very limited abilities to handle other aspects of user-agent interactions, like the ability to coordinate speaking turns with the users. 
%Indeed to coordinate their speaking turns, the agents use simple rules: never speak at the same time the user is speaking, when the user barges in, interrupt to let the user speak, and take the turn as soon as possible after the user's end of turn. But real conversations are more complex than that, speech overlaps are not so rare and conflictual situations, were both participants wants to have the floor at the same time are also common. Moreover the duration of silences during transitions between two turns are highly variable. 
%Improving the naturalness of user-agent interactions requires to take into account these characteristic of human conversations.

%In the context of user-agent interactions, 
Several studies have shown the relevance of varying the way the agents take the turn for conveying different personality traits and interpersonal attitude to the user \citep{ter_maat_how_2010,cafaro_effects_2016}. 
Recent work have shown that part of the variability observed in the duration of transitions in human conversations could come from lower level cognitive processes, linked to the sensorimotor coupling that occurs between participants, but only few models of turn-taking account for this, and none of them have been evaluated in the context of user-agent spoken interactions. 
Although we do not neglect the contribution of other cognitive processes in the coordination of speaking turns, that may take place at the same time at different interaction levels, 
%as at other levels of interactions simple reactions \citep{duncan_signals_1972} and projections \citep{sacks_simplest_1974} could occur at the same time, 
in this article we explore the contribution of sensorimotor coupling between the user and the agent in the coordination of speaking turns and discuss several contributions linked to this question. 

First, we propose a new computational model that endows conversational agents with the capability to coordinate their speaking turns in the context of mixed-initiative dialogs. The simulation of agent-agent interactions highlights the ability of the model to reproduce qualitatively the situations observed during human conversations and reported in the literature. 
We also show that the model allows the agent to adapt to its environmental setting.
%by presenting our model and describing its main properties form agent-agent simulations, we want to highlight the ability of our model to make emerge situations that reproduce qualitatively the situations we can observe in human conversations, and we will show that agents controlled by our model display properties of adaptability to the environmnental settings of the agent. 
More importantly, this adaptability is not due to a process explicitly formulated in our model but is a property that emerges from the interaction. 

Secondly, we show the applicability of the model to handle real user-agent interactions. To our best knowledge, emergent models for user-embodied conversational agents are not common and among these existing models, even less models have been applied to real-time user-agent interactions. As a result, current embodied conversational agent architectures are not fully adapted to the implementations of such kind of models. 
As a contribution, we propose a computational architecture, inspired from \cite{kopp_architecture_2014} and \cite{thorisson_mind_1999}, that supports the implementation of both evenemential rule-based models and continuous emergent models. 

The third contribution of the article relates to the experimental analysis of real-time interactions where the agent has to coordinate its turns with the user. This evaluation aimed at analyzing the ability of both our agent and the user to coordinates their turns. 
We measured the impact of modulating the way the agent coordinates its turns (interrupting or waiting the user's end of turn) on the user's experience. 
%We present the results of an experimentation designed to assess 
% This encompass verifying 
%that our agent was able to reliably detect the behavior of the user and also to verify that users were able to reliably perceive the behavior of the agent, a prerequisite for the user being able to coordinate efficiently with the agent. We also measured the impact of modulating the way the agent coordinates its turns (interrupting or waiting the user's end of turn) on the user's experience. 

In the remainder of the article, we firstly present existing work on human and user-agent turn-taking, and justify our approach. We then present our theoretical model and describe its properties based on theoretical agent-agent simulations.  Next, we present the architecture we propose to implement the model in the context of real-time user-agent dialogs, making our turn-taking model work together with components dedicated to the interpretation and the generation of verbal utterances. We finally conclude the article by analysing real-time interactions between an agent controlled by our model and a user. 
