\section{Implementation of the model}
\label{impl}

\subsection{Requirements}

%We want an agent able to coordinate its turn in real-time with the user in a real dialogue scenario. In this kind of interaction, 
Turn-taking management is one of the numerous processes an ECA has to manage to be engaged in a natural interaction with the user. 
Among others, turn-taking management is closely linked with 
the dialogue management, including natural language understanding and generation, 
the grounding which necessicates to generate and to react to the different types of feedback the user can produce, such as backchannels, 
and also the control of the engagement witht the user. 
Existing ECA architectures provide the core functionalities required to support these process, at least partially. 
Our aim was to propose a solution for integrating our model into existing ECA architecture.
However, the continuous nature of the two main components of the model that are executed in parallel, implies some adaptation of existing architectures. 

%and to bring a stone to the buiding of ECA able to sustain mixed initiative dialogue in a very human-like fashion. 

%For example, apart from the turn-taking coordination, the agent has to be able to interpret the utterance of the user, formulated in natural language, and to generate an utterance in response, managing grounding by interpreting and generating the different types of feedback the user can produce, such as backchannels, perceiving the engagement of the user or showing its own engagement in the dialogue. 
%In order to manage the concurrent execution of all these processes, we would like to use an existing modular architecture, where we could integrate our turn-taking model without adapting it to work with the other components of this architecture.

%On the contrary, our model perceives and controls the actions of the agent in a continuous fashion : actions of the agent are continuously modulated rather than triggered by decision modules, and the perception and decision processes run concurrently rather than sequentially. 
%These requirements follows the design principle of the recent architecture ASAP. 
%We thus chose the ASAP framework as a basis for the implementation of our model. 

The closest solution suitable for implementing our model is ASAP from \cite{kopp_architecture_2014}.
Nevertheless, some mandatory details lack in the specification of the ASAP architecture. For instance, we need to specify how to manage the concurrent execution of the different decision modules. Several conflicting actions can, for example be sent at the same time to the realizer, which thus necessitates a mechanism to select or merge the actions in order to avoid inconsistencies in the action generation. Another issue is the selection of the multimodal communicative actions from the commands provided by the decision modules, and specifically, how to deal with cases where these commands require to change in real-time the actions generated by the agent. 
%To address these issues we based our solution 
%We wanted also to specify more precisely the communication mecanism between the different modules.
We inspired from some principles of the Ymir architecture to address these issues and extended the ASAP architecture.  
%We will now describe our agent architecture.

\subsection{Architecture overview}

%We used the current repartition given by the ASAP architecture. 
The overall organisation of the architecture, which is composed of six modules, is shown on Figure \ref{overall_archi}. 
Modules dedicated to the perception are: 
the Sensing module receiving sensory data, 
the Behavior Interpretation module, interpreting behavioral patterns such as nodding from the multimodal sensory data, 
and the Function interpreter, interpreting the communicative functions of the behavioral patterns. 
The decision part is composed of three modules, 
the Intent Planner generating the communicative intentions of the agent, 
the Behavior Planner generating the multimodal actions from the communicative intentions and managing the execution of the actions (activating, modulating or interrupting the actions) based on the behavioral patterns perceived by the Behavior Interpreter,
and finally the Behavior realizer. 
% COMMENT: Pierre : Mathieu verifie les fonctions du behavior planner et du behavior realize

\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{figure/impl_schema.pdf}
  \caption{Overall organisation of the architecture.}
  \label{overall_archi}
\end{figure}

Inside each modules, we divided the action control and perception into several submodules, each specialized into the perception of one particular behavioral pattern or communicative act, or into the generation or the control of one particular communicative intention. Following the terminology of \cite{thorisson_mind_1999}, the submodules of the behavior and function interpretation modules were called perceptors, those of the sensing modules the sensors, those of the intent planner and the behavior planner the deciders, and those of the behavior realizer were called realizers. 
Following the Ymir principles, the different submodules run concurrently, each submodule having its own execution frequency. The different execution frequencies will be defined when we will present the implementation of the model. The submodules are also either activated or deactivated like is Ymir.
%following \citep{thorisson_mind_1999}, defining if the submodules currently run (activated) in the architecture or not (deactivated). 
The activation or deactivation of modules depends on the current state of the dialog. 
The fact that these modules are activated or not depends on the state of the dialog, 
For example modules involved in the detection of backchannels  are activated only when the agent is the current speaker. 
The activation or deactivation of the submodules is implemented in the Function interpreter module by the module managers. 

We distinguish two types of decider: intention and action decides. 
Intention deciders belong to the intent planner, and are responsible for the generation of the communicative intentions based on the perception data provided by the function interpretation module. As an output, the intention deciders generate FML data. 
The action deciders take as input the communicative intentions provided by the intention deciders, and the perceptual data provided by the behavior interpretation module. As output, they generate an action command that describes the action to control and the new values of the action parameters, according to the perceptual data and the communicative intention of the agent. 
These commands are then sent to an action scheduler that have the responsibility to transcribe the commands into multimodal motor commands and to manage their concurrent execution. Once the motor commands have been determined, the action scheduler sends them to the realizers. 
We inspired from the Ymir architecture for the conception of the action scheduler. The difference is linked to the management of the continuous multimodal actions: the deciders modulating the actions of the agent, send to the action scheduler a continuous flow of commands. Our action scheduler manages this continuous flow of commands, with the capability of modifying on the fly the multimodal motor commands bound to a particular action, based on the parameters the deciders send to the action scheduler. 
The action scheduler sends then the different motor commands to the corresponding realizers using the BML language. 

\subsection{Implementation}

We now describe our we implemented the different components of our model in the architecture. We illustrate on the figure \ref{impl_modules}, the repartition of the different submodules used in our implementation. 

\begin{figure}
\centering
\includegraphics[width=\linewidth]{figure/impl_equ_dial.pdf}
\caption{Repartition of the components of the model and the different submodules dedicated to the utterance interpretation and generation and the determination of the motivation value.}
\label{impl_modules}
\end{figure}
 

\subsubsection{Components of the model}

The two components of our model are implemented into two different modules of the architecture. According to our model, the user produces a certain behavioral pattern that the agent has to discriminate. In this sense, the user behavior perception component of our model should be implemented in the behavior interpretation module. We separate the user behavior perception component into several independant submodules. First, several submodules compute the partial accumulation rate for each signal produced by the user. These partial accumulation rates are then used by a submodule computing the global accumulation rate ($\alpha(t)$ variable in the model). This global accumulation rate is then used by the submodule in charge of updating the accumulation value according to the equation \ref{perc_int}. 
The accumulation value is then sent to the behavior planner. In the behavior planner is implemented the signal control component of our model. Each signal control equation is implemented in a particular action decider, for an agent managing the pitch and loudness variations of the agent, we would have then two different deciders, one for the pitch and one for the loudness. The fact that the agent can control independently the different signals could be contradictory according to the physical coupling that can exist between different signals. For example, when the loudness is nul, the agent doesn't produce any sound and talking about pitch variation is a nonsense. However, the signal control equations of our model define here more control parameters of the different realizers of the architecture and do not correspond to the real signal values. The management of the different physical couplings that can exist in the agent is done by the different realizers of the architectures. 
Each action decider has two inputs, an input corresponding to the motivation to change its role or not and an input corresponding to the accumulation value of the agent. To our view, the motivation to speak comes from different factors.  If the agent is finishing its utterance and has nothing more to say, it is likely that its motivation to change role will be positive. On the opposite, its motivation will likely be negative. The strength of the motivation, will then be regulated by other modules, for example the interpersonal attitudes of the participants \citep{ter_maat_how_2010,ravenet_conversational_2015} or the importance and nature of the contribution made by the participant \citep{cafaro_effects_2016}. As the nature of the factors influencing the motivation to speak can be different, we chose to implement the determination of the motivation in an independant module of the intent planner, this module updates over time the motivation of the agent and then send an FML data specifying the new motivation value to the behavior planner. 
The action deciders generate as outputs action commands specifying the control parameters of the agent's signals. This command is then sent to the action scheduler that update the motor command of the agent and then send the modifications to the corresponding realizers.

In addition to the submodules implementing our turn-taking model, we added submodules dedicated to the interpretation of the user's natural language in the function interpretation module and to the generation of utterances in the intent planner. 
Once the utterence has been generated by the dedicated submodule, it is sent to a specific submodule of the behavior planner. This submodule decide to send to the action scheduler an action command specifying the utterance to say depending to the constraint that applies to this utterance. When the communicative intention specifies that the utterance should be pronounced immediately, the utterance is sent directly to the action scheduler whether or not the text to speech system is currently pronoucing an utterance. As a result, when the realizer receives the utterance, it cancels the previous utterance and send the new utterance to the text to speech system. On the contrary, when the communicative intention specifies that the utterance should be generated after the current pronounced utterance, the decider waits that the realizer send a feedback specifying that it has finished pronouncing the utterance before sending the new utterance. 
When the realizer receives then a command specifying the generation of an utterance, it verifies the loudness and pitch values sent by the action scheduler. When the loudness value is greater than a threshold value, it transmit the utterance  to the TTS system. Each time the realizer receives new commands specifying the loudness and pitch values, it send the new values to the TTS system that modulate the prosody of the utterance being currently generated. When loudness value received by the realizer becomes lesser than a threshold value, the realizer decides to pause the pronounciation being currently pronounced. 

\subsubsection{Example scenarios}

For the implementation, we focused on the interpretation and modulation of the loudness and pitch of participants. We defined the different equations based on observations we made on a corpus of human interactions. For more details on the way we defined these equations, see \cite{jegou_continuous_2015}.

The implementation of the architecture and the submodules has been realised in JAVA. For this implementation, we fixed the execution frequency values of the different submodules to 10 Hz. For the implementation of the signal control submodules and the determination of the accumulation value, it has been necessary to use numerical simulation algorithms. For the signal control equations, we used the Runge-Kutta 4 method, and for the accumulation value equation, we used the Euler-Ma ruyama method. 

We used the openSmile tool \citep{eyben_recent_2013} for the captation of the loudness and pitch of the participant. The tool mesures and send every 100 ms the loudness and pitch value of the user. The agent has finally a graphical representation. This graphical representation has been implemented using Unity3D. The character has for the moment to different animation, speaking and listening. Those animations are synchronized with the output of the TTS system, such as, when the TTS pronounce an utterance, the speak animation is played and when no utterance is pronounced the listening animation is played. 

In order to be able to test our model we needed incremental speech recognition and synthesis tools. With an incremental speech recognizer, the agent is able to formulate hypothesis on what the user is saying before the latter has finished his utterance. The agent can then, based on the hypotheses plan soonits own utterance and then, depending on his motivation value, choose to interrupt or not the user. For the incremental speech recognition we used the Microsoft Speech API. 

The incremental speech synthesis tools permit to rapidly modify the properties of the utterance pronounced by the agent, such as revising what the agent will say and the prosody of the agent. We mostly used an incremental speech synthesis tools to be able to modulate on the fly the prosody of the utterance pronounced by the agent. For this incremental speech synthesis system we used inpro\_iSS created by \cite{baumann_inpro_2012}. 

We will now demonstrate two examples of real-time interactions between the user and the agent. In these example, the agent is able to interpret the sentence pronounced by the user and chose which sentence it will pronounce in response to the user's utterance. We formulated adjacent pair to determine which sentence the agent will pronounce in responce to the different utterances of the user. 

For each examples, we will show the loudness profile of the interaction transcription, represented as green curves and will show below the transcription of the dialog between both participants. We mention in the figure when the agent is certain about the user's utterance and plan the utterance it will say by the ``detect" annotation. 

The figure \ref{sc_1} shows a first example when the agent pronounce an utterance and the user barges-in. In this case the agent strongly want to keep the turn ($m_{sp}=-1.0$). As a result, the agent raises its loudness between the expression ``mount" and ``a shelf" and the loudness remains high until the agent pronounces the word ``red". This can be explained by an accumulation variable becoming positive when the user barges-in, meaning that the agent is perceiving that the user is trying to take the turn. This positive accumulation value is sent from the behavior interpretation module to the behavior planner. The submodules controlling the agents actions raises then their attractors, making the control parameter raising and then finally making the loudness of the utterance of the agent raise. Contrarily, the figure \ref{sc_2}, shows a case where the agent has a weak motivation to speak making him interrupt in response to the user's signals.

\begin{figure}
\centering
\includegraphics[width=\linewidth]{figure/volume_transcript_1_1.png}
\caption{Illustration of the first scenario, the user tries here to barge-in the agent, the latter does not let the turn to the user.}
\label{sc_1}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=\linewidth]{figure/volume_transcript_1_2.png}
\caption{Illustration of a scenario where the agent interrupts itself in response to the user barge-in.}
\label{sc_2}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=\linewidth]{figure/volume_transcript_2_2.png}
\caption{Illustration of a scenario where the agent waits the end of turn of the user even if it has understood was the latter was saying.}
\label{sc_3}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=\linewidth]{figure/volume_transcript_2_1.png}
\caption{Illustration of a scenario where the agent interrupts the user as soon as it has understood was the user was saying.}
\label{sc_4}
\end{figure}

The figure \ref{sc_3} shows an other example when the user asks a question to the agent, and then agent recognizes the utterance pronounced by the user before the latter has finished to pronounce it. Once the agent has recognized what the user was saying the agent wait the user finished its utterance before interrupting it. In this example, as soon as the agent has recognized what the user was saying, its utterance generation module plan an utterance that is sent to the realizer. Nevertheless, as the loudness command of the realizer is less than a certain threshold value, the utterance is not immediately launched in the TTS. The control parameter provided by the signal control submodules, the loudness parameter depends on the result of the turn-taking submodules. While the user is still speaking, the agent having here a weak motivation to speak, the loudness of the agent remains lesser than the threshold value. When the user has finished the turn, the accumulation value begins to raise towards positive value. When this accumulation value attain a certain (high) value, the agent raise the loudness attractor, making the loudness parameter sent to the realizer higher and higher. As soon as this loudness parameter received by the realizer becomes high than the silence threshold value, the utterance is launched in the realizer.
The fact the agent waits a certain amount of time in this scenario is due to the way we defined the loudness control equations. In these equations, depending on the motivation value, the loudness begin to rise after a certain accumulation value, the lowest the motivation, the higher the accumulation value needed to begin to raise the loudness. If we raise the motivation value, the agent will begin to speak sooner in the previous scenario, and, as an extreme, will interrupt the agent if the agent has a strong motivation value to speak that makes him not waiting a positive accumulation value before taking the turn as shown on the figure \ref{sc_4}. 


