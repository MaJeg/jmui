
@inproceedings{ter_maat_how_2010,
    title = {How turn-taking strategies influence users’ impressions of an agent},
    urldate = {2014-03-17},
    booktitle = {Intelligent {Virtual} {Agents}},
    publisher = {Springer},
    author = {Ter Maat, Mark and Truong, Khiet P. and Heylen, Dirk},
    year = {2010},
    pages = {441--453}
}

@inproceedings{bevacqua_effects_2014,
    title = {Effects of {Coupling} in {Human}-{Virtual} {Agent} {Body} {Interaction}},
    urldate = {2015-03-19},
    booktitle = {Intelligent {Virtual} {Agents} 2014},
    address = {Boston, MA},
    author = {Bevacqua, Elisabetta and Stankovi{\'c}, Igor and Maatallaoui, Ayoub and N\'ed\'elec, Alexis and De Loor, Pierre},
    year = {2014},
    pages = {54--63}
}

@inproceedings{levitan_entrainment_2015,
    title = {Entrainment and {Turn}-{Taking} in {Human}-{Human} {Dialogue}},
    booktitle = {2015 {AAAI} {Spring} {Symposium} {Series}},
    author = {Levitan, Rivka and Benus, Stefan and Gravano, Agust{\`\i}n and Hirschberg, Julia},
    month = mar,
    year = {2015},
}

@article{sacks_simplest_1974,
	title = {A simplest systematics for the organization of turn-taking for conversation},
	journal = {Language},
	author = {Sacks, Harvey and Schegloff, Emanuel A. and Jefferson, Gail},
	year = {1974},
	pages = {696--735}
}

@book{clark_using_1996,
	title = {Using {Language}},
	language = {Anglais},
	publisher = {Cambridge, England: Cambridge University Press},
	author = {Clark, Herbert H.},
	year = {1996}
}

@incollection{thorisson_modeling_2008,
	title = {Modeling multimodal communication as a complex system},
	booktitle = {Modeling {Communication} with {Robots} and {Virtual} {Humans}},
	publisher = {Springer},
	author = {Th\'orisson, Kristinn R.},
	year = {2008},
	pages = {143--168}
}

@article{stivers_universals_2009,
	title = {Universals and cultural variation in turn-taking in conversation},
	volume = {106},
	number = {26},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Stivers, Tanya and Enfield, N. J. and Brown, Penelope and Englert, Christina and Hayashi, Makoto and Heinemann, Trine and Hoymann, Gertie and Rossano, Federico and Ruiter, Jan Peter de and Yoon, Kyung-Eun and Levinson, Stephen C.},
	month = jun,
	year = {2009},
	pages = {10587--10592}
}

@article{balentine_debouncing_1997,
	title = {Debouncing the speech button: {A} sliding capture window device for synchronizing turn-taking},
	volume = {2},
	shorttitle = {Debouncing the speech button},
	abstract = {When human beings converse, they alternate between talking and listening. Participating in such turntaking behaviors is more difficult for machines that use speech recognition to listen and speech output to talk. This paper describes an algorithm for managing such turn-taking through the use of a sliding capture window. The device is specific to discrete speech recognition technologies that do not have access to echo cancellation. As such, it addresses those inexpensive applications that suffer the most from turn-taking errors—providing a “speech button” that stabilizes the interface. Correcting for short-lived turn-taking errors can be thought of as “debouncing” the button. An informal study based on ten subjects using a voice dialing application illuminates the design.},
	number = {1},
	urldate = {2014-03-17},
	journal = {International Journal of Speech Technology},
	author = {Balentine, Bruce E. and Ayer, Colin M. and Miller, Clint L. and Scott, Brian L.},
	month = may,
	year = {1997},
	keywords = {Artificial Intelligence (incl. Robotics), discrete speech recognition, Interactivity, Signal, Image and Speech Processing, Social Sciences, general, speech button, temporal cues, Turn-taking},
	pages = {7--19}
}

@inproceedings{ward_root_2005,
	title = {Root causes of lost time and user stress in a simple dialog system},
	booktitle = {Proceedings of {INTERSPEECH} 2005},
	author = {Ward, Nigel G. and Rivera, Anais G. and Ward, Karen and Novick, David G.},
	year = {2005}
}

@article{raux_optimizing_2012,
	title = {Optimizing the {Turn}-taking {Behavior} of {Task}-oriented {Spoken} {Dialog} {Systems}},
	volume = {9},
	number = {1},
	journal = {ACM Transaction on Speech Language Processing},
	author = {Raux, Antoine and Eskenazi, Maxine},
	month = may,
	year = {2012},
	pages = {1:1--1:23}
}

@incollection{jonsdottir_distributed_2013,
	title = {A {Distributed} {Architecture} for {Real}-time {Dialogue} and {On}-task {Learning} of {Efficient} {Co}-operative {Turn}-taking},
	booktitle = {Coverbal Synchrony in Human-Machine Interaction},
	author = {Jonsdottir, Gudny Ragna and Th\'orisson, Kristinn R.},
	year = {2013},
	pages = {293--323},
	editor={Nick Campbell},
	publisher = {CRC Press}
}

@incollection{oconnell_turntaking_2008,
	series = {Cognition and {Language}: {A} {Series} in {Psycholinguistics}},
	title = {Turn-taking},
	booktitle = {Communicating with {One} {Another}},
	publisher = {Springer New York},
	author = {O’Connell, Daniel C. and Kowal, Sabine},
	year = {2008},
	pages = {1--13}
}

@inproceedings{selfridge_continuously_2013,
	title = {Continuously predicting and processing barge-in during a live spoken dialogue task},
	booktitle = {Proceedings of the {SIGDIAL} 2013 {Conference}},
	author = {Selfridge, Ethan and Arizmendi, Iker and Heeman, Peter and Williams, Jason},
	year = {2013},
	pages = {384--393}
}

@article{witt_modeling_2014,
	title = {Modeling user response timings in spoken dialog systems},
	volume = {18},
	number = {2},
	journal = {International Journal of Speech Technology},
	author = {Witt, Silke},
	year = {2014},
	pages = {231--243},
abstract = {This article is concerned with optimizing human-machine turn-taking. In particular, the article covers an in-depth analysis of the timings when users respond to system query in spoken dialog systems. The goal of this work is to obtain a broad understanding of such timing patterns independent of dialog system type and dialog state context. Therefore, the analysis was based on a large volume of data both from a number of deployed spoken dialog system and an experimental study. The data from the experimental study showed that too short timeout settings can cause the system to interrupt a user and thus cause turn-taking problems. Next, the response timing patterns both during a system prompt as well as after prompt-end were analyzed for a number of different question types. It is shown that user responses while the system is playing a prompt (aka ‘barge-in’) tend to occur in the range of 10–25 \% of all user responses, where the exact percentage of barge-in is context-dependent. It was also found that the timing of user responses after a system finishes speaking always follows the same uni-modal pattern independent of system domain and question type. This pattern can be modeled with a rational distribution. Based on these findings, a probabilistic response time model is presented, that allows calculating the likelihood of a user response at any time in a system. This response timing model can be used for multiple purposes, among them timeout setting optimization.}
}

@article{heldner_pauses_2010,
	title = {Pauses, gaps and overlaps in conversations},
	volume = {38},
	number = {4},
	urldate = {2014-09-24},
	journal = {Journal of Phonetics},
	author = {Heldner, Mattias and Edlund, Jens},
	month = oct,
	year = {2010},
	pages = {555--568}
}

@inproceedings{ohshima_conversational_2015,
	title = {A conversational robot with vocal and bodily fillers for recovering from awkward silence at turn-takings},
booktitle = {IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN)},
	author = {Ohshima, Naoki and Kimijima, Keita and Yamato, Junji and Mukawa, Naoki},
	year = {2015},
	pages = {325--330}
}

@article{duncan_signalling_1974,
	title = {On signalling that it's your turn to speak},
	volume = {10},
	abstract = {The operation of a “speaker-state signal” in two-person, face-to-face conversations is hypothesized. Display of this signal by an auditor appears to indicate, among other things, that he is claiming the speaking turn, differentiating this action from a “back-channel behavior” by which he merely acknowledges some portion of the speaker's message. The signal also appears to play a part in the resolution of situations in which both participants simultaneously claim the speaking turn. The signal is defined as the display of at least one of a set of four behavioral cues, two in paralanguage and two in body motion.},
	number = {3},
	urldate = {2014-03-20},
	journal = {Journal of Experimental Social Psychology},
	author = {Duncan, Starkey and Niederehe, George},
	month = may,
	year = {1974},
	pages = {234--247}

}

@article{duncan_signals_1972,
	title = {Some signals and rules for taking speaking turns in conversations},
	volume = {23},
	number = {2},
	journal = {Journal of Personality and Social Psychology},
	author = {Duncan, Starkey},
	year = {1972},
	pages = {283--292}

}

@article{mondada_multimodal_2007,
	title = {Multimodal resources for turn-taking: pointing and the emergence of possible next speakers},
	volume = {9},
	number = {2},
	urldate = {2015-03-23},
	journal = {Discourse Studies},
	author = {Mondada, L.},
	month = apr,
	year = {2007},
	pages = {194--225},

}

@article{schegloff_overlapping_2000,
	title = {Overlapping talk and the organization of turn-taking for conversation},
	volume = {29},
	number = {01},
	urldate = {2014-05-05},
	journal = {Language in society},
	author = {Schegloff, Emanuel A.},
	year = {2000},
	pages = {1--63},

}

@article{french_turn-competitive_1983,
	title = {Turn-competitive incomings},
	volume = {7},
	abstract = {A recurrent feature of multi-party conversation is that one speaker comes in prior to the completion of another's turn and can be heard as directly competing with the other for possession of the turn. That is, the incomer can be heard as wanting the floor to himself not when the current speaker has finished but now, at this point in the conversation. Our analysis reveals that in managing talk of this kind participants, methodically produce and monitor for certain prosodic features of speech. These features, which have hitherto received scant attention in analyses of interaction, involve pitch-height, tempo and loudness variations. By deploying these prosodic features participants can constitute their incomings as competitive for the turn irrespective of the lexico-syntactic or illocutionary characteristics of their talk.},
	number = {1},
	urldate = {2014-03-11},
	journal = {Journal of Pragmatics},
	author = {French, Peter and Local, John},
	month = feb,
	year = {1983},
	pages = {17--38}	
}

@article{ford_interactional_1996,
	title = {Interactional units in conversation: syntactic, intonational, and pragmatic resources for the management of turns},
	abstract = {Official Full-Text Publication: Interactional units in conversation: syntactic, intonational, and pragmatic resources for the management of turns on ResearchGate, the professional network for scientists.},
	journal = {Studies in interactional sociolinguistics},
	author = {Ford, C. and Thompson, S. A.},
	month = jan,
	year = {1996},
	pages = {134--184}
}

@article{de_ruiter_projecting_2006,
	title = {Projecting the end of a speaker's turn: {A} cognitive cornerstone of conversation},
	shorttitle = {Projecting the end of a speaker's turn},
	urldate = {2014-11-07},
	journal = {Language},
	author = {De Ruiter, Jan Peter and Mitterer, Holger and Enfield, Nick J.},
	year = {2006},
	volume={82},
	number={3},
	pages = {515--535},

}

@article{bogels_listeners_2015,
	title = {Listeners use intonational phrase boundaries to project turn ends in spoken interaction},
	volume = {52},
	journal = {Journal of Phonetics},
	author = {B\"ogels, Sara and Torreira, Francisco},
	month = sep,
	year = {2015},
	pages = {46--57},

}

@article{pierrehumbert_meaning_1990,
	series = {Bradford {Books}, {MIT} {Press}, {Cambridge} {MA}.},
	title = {The meaning of intonational contours in the interpretation of discourse},
	journal = {Intentions in Communication},
	author = {Pierrehumbert, Janet B. and Hirschberg, Julia},
	year = {1990},
	pages = {271--311},

}

@article{gravano_turn-taking_2011,
	title = {Turn-taking cues in task-oriented dialogue},
	volume = {25},
	number = {3},
	urldate = {2014-03-17},
	journal = {Computer Speech \& Language},
	author = {Gravano, Agust{\'\i}n and Hirschberg, Julia},
	year = {2011},
	pages = {601--634},

}


@article{benus_pragmatic_2011,
	title = {Pragmatic aspects of temporal accommodation in turn-taking},
	volume = {43},
	abstract = {This study investigates the relationship between the variability in the temporal alignment of turn initiations and the pragmatics of interpersonal communication. The data come from spontaneous, task-oriented dialogues in Standard American English. In addition to analyzing the temporal aspects of turn-taking behavior in general, we focus on the timing of turn-initial single word grounding responses such as mmhm, okay, or yeah, and conversational fillers such as um or uh. Based on qualitative and quantitative analyses of temporal and rhythmic alignment patterns, we propose that these patterns are linked to the achievement of pragmatic goals by interlocutors. More specifically, we examine the role of timing in establishing common ground, and test the hypothesis that the degree of accommodation to temporal and metrical characteristics of an interlocutor's speech is one aspect of turn-taking behavior that signals asymmetrical dominance relationships between interlocutors. Our results show that dominance relationships linked to floor-control, as well as mutual common ground, are pragmatically constructed in part through the accommodation patterns in timing of turn-initial single word utterances.},
	number = {12},
	urldate = {2014-03-11},
	journal = {Journal of Pragmatics},
	author = {Be\v{n}u\v{s}, \v{S}tefan and Gravano, Agust{\'\i}n and Hirschberg, Julia},
	month = sep,
	year = {2011},
	keywords = {Accommodation, Dominance, Grounding response, Rhythm, Turn-taking},
	pages = {3001--3027},

}

@article{kendon_functions_1967,
	title = {Some functions of gaze-direction in social interaction},
	volume = {26},
	journal = {Acta Psychologica},
	author = {Kendon, Adam},
	year = {1967},
	pages = {22--63},

}

@article{grosjean_using_1996,
	title = {Using {Prosody} to {Predict} the {End} of {Sentences} in {English} and {French}: {Normal} and {Brain}-{Damaged} {Subjects}},
	volume = {11},
	issn = {0169-0965},
	shorttitle = {Using {Prosody} to {Predict} the {End} of {Sentences} in {English} and {French}},
	abstract = {This study investigates the phenomenon that listeners of English were surprisingly accurate at predicting the temporal end of a sentence when only given the part up to the "potentially last word," that is a noun before an optional prepositional phrase of varying lengths. Results of four experiments using either French or English are given. (35 references) (Author/CK)},
	language = {en},
	urldate = {2014-10-01},
	journal = {Language and Cognitive Processes},
	author = {Grosjean, Francois and Hirt, Cendrine},
	year = {1996},
	number={1-2},
	keywords = {Auditory Perception, Auditory Stimuli, English (Second Language), French, Language Processing, Listening Comprehension, Models, Predictor Variables, Sentence Structure, Speech Communication},
	pages = {107--134}
}

@inproceedings{bohus_decisions_2011,
	title = {Decisions {About} {Turns} in {Multiparty} {Conversation}: {From} {Perception} to {Action}},
	shorttitle = {Decisions {About} {Turns} in {Multiparty} {Conversation}},
	abstract = {We present a decision-theoretic approach for guiding turn taking in a spoken dialog system operating in multiparty settings. The proposed methodology couples inferences about multiparty conversational dynamics with assessed costs of different outcomes, to guide turn-taking decisions. Beyond considering uncertainties about outcomes arising from evidential reasoning about the state of a conversation, we endow the system with awareness and methods for handling uncertainties stemming from computational delays in its own perception and production. We illustrate via sample cases how the proposed approach makes decisions, and we investigate the behaviors of the proposed methods via a retrospective analysis on logs collected in a multiparty interaction study.},
	urldate = {2014-11-12},
	booktitle = {Proceedings of the 13th {International} {Conference} on {Multimodal} {Interfaces}},
	author = {Bohus, Dan and Horvitz, Eric},
	year = {2011},
	keywords = {decision-theoretic approach, floor management, Gaze, multimodal systems, multiparty interaction, multiparty turn taking, situated interaction, Speech, spoken dialog},
	pages = {153--160},

}

@inproceedings{cassell_embodiment_1999,
	title = {Embodiment in conversational interfaces},
	author = {Cassell, J. and Bickmore, T. and Billinghurst, M. and Campbell, L. and Chang, K and Vilhjálmsson, H. and Yan, H.},
	booktitle = {Proceedings of the SIGCHI conference on Human Factors in Computing Systems},
	year = {1999},
	pages = {520--527}
}

@article{thorisson_natural_2002,
	title = {Natural turn-taking needs no manual: {Computational} theory and model, from perception to action},
	volume = {19},
	shorttitle = {Natural turn-taking needs no manual},
	urldate = {2014-01-13},
	journal = {Multimodality in language and speech systems},
	author = {Th\'orisson, Kristinn R.},
	year = {2002},

}

@inproceedings{de_kok_multimodal_2009,
	address = {New York, NY, USA},
	series = {{ICMI}-{MLMI} '09},
	title = {Multimodal {End}-of-turn {Prediction} in {Multi}-party {Meetings}},
	abstract = {One of many skills required to engage properly in a conversation is to know the appropiate use of the rules of engagement. In order to engage properly in a conversation, a virtual human or robot should, for instance, be able to know when it is being addressed or when the speaker is about to hand over the turn. The paper presents a multimodal approach to end-of-speaker-turn prediction using sequential probabilistic models (Conditional Random Fields) to learn a model from observations of real-life multi-party meetings. Although the results are not as good as expected, we provide insight into which modalities are important when taking a multimodal approach to the problem based on literature and our own results.},
	urldate = {2016-11-10},
	booktitle = {Proceedings of the 2009 {International} {Conference} on {Multimodal} {Interfaces}},
	publisher = {ACM},
	author = {de Kok, Iwan and Heylen, Dirk},
	year = {2009},
	keywords = {end-of-turn prediction, Multimodal, probabilistic model},
	pages = {91--98},

}

@inproceedings{huang_multimodal_2011,
	address = {Richland, SC},
	series = {{AAMAS}'11},
	title = {A {Multimodal} {End}-of-turn {Prediction} {Model}: {Learning} from {Parasocial} {Consensus} {Sampling}},
	isbn = {978-0-9826571-7-1},
	shorttitle = {A {Multimodal} {End}-of-turn {Prediction} {Model}},
	abstract = {Virtual human, with realistic behaviors and social skills, evoke in users a range of social behaviors normally only seen in human face-to-face interactions. One of the key challenges in creating such virtual humans is to give them human-like conversational skills, such as turn-taking skill. In this paper, we propose a multimodal end-of-turn prediction model. Instead of recording face-to-face conversation data, we collect the turn-taking data using Parasocial Consensus Sampling (PCS) framework. Then we analyze the relationship between verbal and nonverbal features and turn-taking behaviors based on the consensus data and show how these features influence the time people use to take turns. Finally, we present a probabilistic multimodal end-ofturn prediction model, which enables virtual humans to make real-time turn-taking predictions. The result shows that our model achieves a higher accuracy than previous methods did.},
	urldate = {2015-12-11},
	booktitle = {The 10th {International} {Conference} on {Autonomous} {Agents} and {Multiagent} {Systems} - {Volume} 3},
	author = {Huang, Lixing and Morency, Louis-Philippe and Gratch, Jonathan},
	year = {2011},
	keywords = {Multimodal, parasocial consensus, Turn-taking, virtual human},
	pages = {1289--1290}
}

@article{de_vault_incremental_2011,
	title = {Incremental interpretation and prediction of utterance meaning for interactive dialogue},
	volume = {2},
	number = {1},
	journal = {Dialogue \& Discourse},
	author = {De Vault, David and Sagae, Kenji and Traum, David},
	year = {2011},
	pages = {143--170}
}

@article{reidsma_continuous_2011,
	title = {Continuous interaction with a virtual human},
	volume = {4},
	abstract = {This paper presents our progress in developing a Virtual Human capable of being an attentive speaker. Such a Virtual Human should be able to attend to its interaction partner while it is speaking—and modify its communicative behavior on-the-fly based on what it observes in the behavior of its partner. We report new developments concerning a number of aspects, such as scheduling and interrupting multimodal behavior, automatic classification of listener responses, generation of response eliciting behavior, and strategies for generating appropriate reactions to listener responses. On the basis of this progress, a task-based setup for a responsive Virtual Human was implemented to carry out two user studies, the results of which are presented and discussed in this paper.},
	number = {2},
	journal = {Journal on Multimodal User Interfaces},
	author = {Reidsma, Dennis and de Kok, Iwan and Neiberg, Daniel and Pammi, Sathish Chandra and van Straalen, Bart and Truong, Khiet and van Welbergen, Herwin},
	month = jul,
	year = {2011},
	keywords = {Attentive speaking, Continuous interaction, Image Processing and Computer Vision, Listener responses, Signal, Image and Speech Processing, User Interfaces and Human Computer Interaction, virtual humans},
	pages = {97--118}
}

@article{cutler_analysis_1986,
	title = {On the analysis of prosodic turn-taking cues},
	urldate = {2016-04-27},
	journal = {Intonation in discourse},
	author = {Cutler, Anne and Pearson, Mark},
	year = {1986},
	pages = {139--156}
}

@article{oconnell_turn-taking_1990,
	title = {Turn-taking : {A} critical analysis of the research tradition},
	volume = {19},
	abstract = {In the following we present a radical critique of the assumptions, concepts, methods, statistics and interpretation of data, and theories that have characterized the recent research tradition concerned with turn-taking. The principal representative of this tradition is the “simplest systematics” of Sacks, Schegloff, and Jefferson (1974). Attempts to describe the generalizable properties of turn-taking have quite inappropriately and unsuccessfully been limited for the most part to formal approaches that have deliberately excluded considerations of conversational content and purpose. We start instead from the assumption that the ultimate criterion for the success of a conversation is not “the smooth interchange of speaking turns” (Cutler \& Pearson, 1986, p. 139) or any other prescriptive ideal, but the fulfillment of the purposes entertained by two or more interlocutors. Our approach is that of a psychology of language use based on Bühler (1927; 1934/1982) and Rommetveit (1974). The emphasis is deliberately placed on social aspects of language as means of communication.},
	number = {6},
	journal = {Journal of Psycholinguistic Research},
	author = {O'Connell, Daniel C. and Kowal, Sabine and Kaltenbacher, Erika},
	month = nov,
	year = {1990},
	keywords = {Cognitive Psychology, Psycholinguistics},
	pages = {345--373}
}

@article{beattie_interruption_1981,
  title={Interruption in conversational interaction, and its relation to the sex and status of the interactants},
  author={Beattie, Geoffrey W},
  journal={Linguistics},
  volume={19},
  number={1-2},
  pages={15--36},
  year={1981}
}

@incollection{clancy_co-constructed_2015,
	address = {Cambridge},
	title = {Co-constructed turn-taking},
	isbn = {978-1-139-05749-3},
	urldate = {2016-04-15},
	booktitle = {Corpus {Pragmatics}},
	publisher = {Cambridge University Press},
	author = {Clancy, Brian and McCarthy, Michael},
	editor = {Aijmer, Karin and Rühlemann, Christoph},
	year = {2015},
	pages = {430--453}
}

@inproceedings{cafaro_effects_2016,
	title = {The {Effects} of {Interrupting} {Behavior} on {Interpersonal} {Attitude} and {Engagement} in {Dyadic} {Interactions}},
	urldate = {2016-06-28},
	booktitle = {Proceedings of the 2016 {International} {Conference} on {Autonomous} {Agents} \& {Multiagent} {Systems}},
	publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
	author = {Cafaro, Angelo and Glas, Nadine and Pelachaud, Catherine},
	year = {2016},
	pages = {911--920}
}

@inproceedings{lessmann_towards_2004,
	address = {New-York, August 19--23},
	title = {Towards a cognitively motivated processing of turn-taking signals for the embodied conversational agent {Max}},
	abstract = {Max is a human-size conversational agent that employs synthetic speech, gesture, gaze, and facial display to act in cooperative construction tasks taking place in immersive virtual reality. In the mixed-initiative dialogs involved in our research scenario, turn-taking abilities and dialog competences play a crucial role for Max to appear as a convincing multimodal communication partner. The way how they rely on Max’s perception of the user and, in special, how turn-taking signals are handled in the agent’s cognitive architecture is the focus of this paper. stage, the discourse is influenced by the situational context, e.g., the mutual consent on individual parts employed so far, the state of the ongoing assembly, or the outcome of a user action. 1.},
	booktitle = {Proceedings of the {Workshop} {Embodied} {Conversational} {Agents}: {Balanced} {Perception} and {Action}},
	publisher = {ACM Press},
	author = {Le{\ss}mann, Nadine and Kranstedt, Alfred and Wachsmuth, Ipke},
	year = {2004},
	pages = {65}
}

@inproceedings{thorisson_multiparty_2010,
	title = {A multiparty multimodal architecture for realtime turntaking},
	urldate = {2014-03-12},
	booktitle = {Intelligent {Virtual} {Agents}},
	publisher = {Springer},
	author = {Th\'orisson, Kristinn R. and Gislason, Olafur and Jonsdottir, Gudny Ragna and Th\'orisson, Hrafn Th},
	year = {2010},
	pages = {350--356}
}

@inproceedings{novick_coordinating_1996,
	title = {Coordinating turn-taking with gaze},
	volume = {3},
	abstract = {Explores the role of gaze in coordinating turn-taking in mixed-initiative conversation, and specifically how gaze indicators might be usefully modeled in computational dialogue systems. We analyzed about 20 minutes of videotape of eight dialogues by four pairs of subjects performing a simple face-to-face cooperative laboratory task. We extend previous studies by explicating gaze patterns in face-to-face conversations, formalizing the most frequent pattern as a computational model of turn-taking and testing the model through an agent-based simulation. Prior conversation simulations of conversational control acts relied on abstract speech-act representations of control. This study advances the computational account of dialogue through simulation of direct physical expression of gaze to coordinate conversational turns},
	booktitle = {{Fourth} {International} {Conference} on {Spoken} {Language}, 1996. {ICSLP} 96. {Proceedings}},
	author = {Novick, D.G. and Hansen, B. and Ward, K.},
	month = oct,
	year = {1996},
	keywords = {abstract speech-act representations, agent-based simulation, behavioural sciences, behavioural sciences computing, Communication system control, computational dialogue systems, Computational modeling, conversational control acts, conversational turn-taking, cooperative systems, Coordination, digital simulation, face-to-face conversations, face-to-face cooperative laboratory task, Frequency, gaze indicators, gaze patterns, Laboratories, mixed-initiative conversation, Monitoring, Natural languages, Performance analysis, physical expression, Physics computing, Power system modeling, Software agents, Testing, videotape},
	pages = {1888--1891 vol.3}
}

@article{magyari_prediction_2012,
	title = {Prediction of {Turn}-{Ends} {Based} on {Anticipation} of {Upcoming} {Words}},
	volume = {3},
	number={376},
	abstract = {During conversation listeners have to perform several tasks simultaneously. They have to comprehend their interlocutor’s turn, while also having to prepare their own next turn. Moreover, a careful analysis of the timing of natural conversation reveals that next speakers also time their turns very precisely. This is possible only if listeners can predict accurately when the speaker’s turn is going to end. But how are people able to predict when a turn-ends? We propose that people know when a turn-ends, because they know how it ends. We conducted a gating study to examine if better turn-end predictions coincide with more accurate anticipation of the last words of a turn. We used turns from an earlier button-press experiment where people had to press a button exactly when a turn-ended. We show that the proportion of correct guesses in our experiment is higher when a turn’s end was estimated better in time in the button-press experiment. When people were too late in their anticipation in the button-press experiment, they also anticipated more words in our gating study. We conclude that people made predictions in advance about the upcoming content of a turn and used this prediction to estimate the duration of the turn. We suggest an economical model of turn-end anticipation that is based on anticipation of words and syntactic frames in comprehension.},
	journal = {Frontiers in Psychology},
	author = {Magyari, Lilla and de Ruiter, J. P.},
	month = oct,
	year = {2012}
}

@article{riest_anticipation_2015,
	title = {Anticipation in turn-taking: mechanisms and information sources},
	volume = {6},
	abstract = {During conversations participants alternate smoothly between speaker and hearer roles with only brief pauses and overlaps. There are two competing types of accounts about how conversationalists accomplish this: (a) the signaling approach and (b) the anticipatory (‘projection’) approach. We wanted to investigate, first, the relative merits of these two accounts, and second, the relative contribution of semantic and syntactic information to the timing of next turn initiation. We performed three button-press experiments using turn fragments taken from natural conversations to address the following questions: (a) Is turn-taking predominantly based on anticipation or on reaction, and (b) what is the relative contribution of semantic and syntactic information to accurate turn-taking. In our first experiment we gradually manipulated the information available for anticipation of the turn end (providing information about the turn end in advance to completely removing linguistic information). The results of our first experiment show that the distribution of the participants’ estimation of turn-endings for natural turns is very similar to the distribution for pure anticipation. We conclude that listeners are indeed able to anticipate a turn-end and that this strategy is predominantly used in turn-taking. In Experiment 2 we collected purely reacted responses. We used the distributions from Experiments 1 and 2 together to estimate a new dependent variable called Reaction Anticipation Proportion. We used this variable in our third experiment where we manipulated the presence vs. absence of semantic and syntactic information by low-pass filtering open-class and closed class words in the turn. The results suggest that for turn-end anticipation, both semantic and syntactic information are needed, but that the semantic information is a more important anticipation cue than syntactic information.},
	urldate = {2015-03-03},
	journal = {Language Sciences},
	author = {Riest, Carina and Jorschick, Annett B. and de Ruiter, Jan P.},
	year = {2015},
	keywords = {anticipation, Conversation, reaction, Timing, Turn-taking},
	pages = {89}
}

@article{schlangen_reaction_2006,
	title = {From reaction to prediction: {Experiments} with computational models of turn-taking},
	shorttitle = {From reaction to prediction},
	urldate = {2014-11-12},
	journal = {Proceedings of Interspeech 2006, Panel on Prosody of Dialogue Acts and Turn-Taking},
	author = {Schlangen, David},
	year = {2006}
}

@inproceedings{ravenet_conversational_2015,
	title = {Conversational {Behavior} {Reflecting} {Interpersonal} {Attitudes} in {Small} {Group} {Interactions}},
	author = {Ravenet, Brian and Cafaro, Angelo and Biancardi, Beatrice and Ochs, Magalie and Pelachaud, Catherine},
	volume = {9238},
	urldate = {2015-11-27},
	booktitle = {Intelligent {Virtual} {Agents}: 15th {International} {Conference}, {IVA} 2015, {Delft}, {The} {Netherlands}, {August} 26-28, 2015, {Proceedings}},
	publisher = {Springer},
	year = {2015},
	pages = {375}
}

@article{wilson_oscillator_2005,
	title = {An oscillator model of the timing of turn-taking},
	volume = {12},
	abstract = {When humans talk without conventionalized arrangements, they engage in conversation—that is, a continuous and largely nonsimultaneous exchange in which speakers take turns. Turn-taking is ubiquitous in conversation and is the normal case against which alternatives, such as interruptions, are treated as violations that warrant repair. Furthermore, turn-taking involves highly coordinated timing, including a cyclic rise and fall in the probability of initiating speech during brief silences, and involves the notable rarity, especially in two-party conversations, of two speakers’ breaking a silence at once. These phenomena, reported by conversation analysts, have been neglected by cognitive psychologists, and to date there has been no adequate cognitive explanation. Here, we propose that, during conversation, endogenous oscillators in the brains of the speaker and the listeners become mutually entrained, on the basis of the speaker’s rate of syllable production. This entrained cyclic pattern governs the potential for initiating speech at any given instant for the speaker and also for the listeners (as potential next speakers). Furthermore, the readiness functions of the listeners are counterphased with that of the speaker, minimizing the likelihood of simultaneous starts by a listener and the previous speaker. This mutual entrainment continues for a brief period when the speech stream ceases, accounting for the cyclic property of silences. This model not only captures the timing phenomena observed in the literature on conversation analysis, but also converges with findings from the literatures on phoneme timing, syllable organization, and interpersonal coordination.},
	language = {en},
	number = {6},
	urldate = {2014-03-12},
	journal = {Psychonomic Bulletin \& Review},
	author = {Wilson, Margaret and Wilson, Thomas P.},
	month = dec,
	year = {2005},
	keywords = {Cognitive Psychology},
	pages = {957--968}
}


@article{goldberg_interrupting_1990,
	title = {Interrupting the discourse on interruptions: {An} analysis in terms of relationally neutral, power- and rapport-oriented acts},
	volume = {14},
	number = {6},
	journal = {Journal of Pragmatics},
	author = {Goldberg, Julia A.},
	month = dec,
	year = {1990},
	pages = {883--903}
}


@inproceedings{selfridge_bidding_2009,
	title = {A bidding approach to turn-taking},
	urldate = {2016-11-07},
	booktitle = {1st {International} {Workshop} on {Spoken} {Dialogue} {Systems}},
	author = {Selfridge, Ethan O. and Heeman, Peter A.},
	year = {2009}
}



@article{mcfarland_respiratory_2001,
author={McFarland, David H.},
title = {Respiratory Markers of Conversational Interaction},
journal = {Journal of Speech, Language, and Hearing Research},
volume = {44},
pages = {128-143},
year = {2001}
}

@article{wilson_structure_1986,
author = {Thomas P. Wilson  and  Don H. Zimmerman },
title = {The structure of silence between turns in two party conversation},
journal = {Discourse Processes},
volume = {9},
number = {4},
pages = {375-390},
year = {1986},
abstract = { Turn taking is a fundamental structural feature of social interaction. Three major approaches to describing turn taking have emerged: stochastic, signaling, and sequential‐production models. The first two treat silences between speakers as simple response latencies, whereas the third views silence as generated collaboratively by the parties to the conversation. The simple response‐latency interpretation predicts a distribution of be‐tween‐turn silences that declines monotonically with duration, whereas the sequential‐production model predicts a periodic pattern of peaks and valleys, with an overall decline in the heights of the peaks as duration increases. Analysis of the frequency distributions of durations of silences between speakers in two‐party conversations finds the periodic structure predicted by the sequential‐production model. The finding is interpreted as supporting a view of social interaction as a fundamentally collaborative activity. }
}

@inproceedings{bailly_pauses_2012,
  TITLE = {Pauses and respiratory markers of the structure of book reading},
  AUTHOR = {Bailly, G{\'e}rard and Gouvernayre, C{\'e}cilia},
  BOOKTITLE = {{13th Annual Conference of the International Speech Communication Association (InterSpeech 2012)}},
  ADDRESS = {Portland, United States},
  YEAR = {2012},
  KEYWORDS = {prediction of pause locations and durations ; respiration ; pause ; prosody}
}

@article{ikegami_turn-taking_2007,
	title = {Turn-taking {Interaction} as a {Cooperative} and {Co}-creative {Process}},
	volume = {30},
	abstract = {In the present article, computer simulation of turn-taking interaction was studied and compared with psychological experiments such as the double TV experiment and synchronous imitation. By introducing the concepts of virtual agents, prediction error and adaptability, the simulation showed that turn-taking dynamics is a cooperative and co-creative process between two agents, which is in accordance with psychological findings. In particular, much attention is paid to styles of motion, as turn-taking with chaotic spatial pattern has developed to be sensitive to it.},
	number = {2},
	journal = {Infant Behavior and Development},
	author = {Ikegami, Takashi and Iizuka, Hiroyuki},
	month = may,
	year = {2007},
	keywords = {Chaos, Co-creation, Cooperation, Evolution, Genetic algorithm, neural network, Prediction, Turn-taking},
	pages = {278--288},

}

@article{bonaiuto_towards_2008,
	title = {Towards a neurocognitive model of turn taking in multimodal dialog},
	urldate = {2014-03-12},
	journal = {Embodied communication in humans and machines},
	author = {Bonaiuto, James and Th\'orisson, K.},
	year = {2008},
	pages = {451--483},

}

@article{haken_theoretical_1985,
	title = {A theoretical model of phase transitions in human hand movements},
	volume = {51},
	abstract = {Earlier experimental studies by one of us (Kelso, 1981a, 1984) have shown that abrupt phase transitions occur in human hand movements under the influence of scalar changes in cycling frequency. Beyond a critical frequency the originally prepared out-of-phase, antisymmetric mode is replaced by a symmetrical, in-phase mode involving simultaneous activation of homologous muscle groups. Qualitavely, these phase transitions are analogous to gait shifts in animal locomotion as well as phenomena common to other physical and biological systems in which new “modes” or spatiotemporal patterns arise when the system is parametrically scaled beyond its equilibrium state (Haken, 1983). In this paper a theoretical model, using concepts central to the interdisciplinary field of synergetics and nonlinear oscillator theory, is developed, which reproduces (among other features) the dramatic change in coordinative pattern observed between the hands.},
	language = {en},
	number = {5},
	urldate = {2014-02-23},
	journal = {Biological Cybernetics},
	author = {Haken, H. and Kelso, J. a. S. and Bunz, H.},
	month = feb,
	year = {1985},
	keywords = {Neurosciences, Zoology},
	pages = {347--356},

}

@article{rio_follow_2014,
	title = {Follow the leader: {Visual} control of speed in pedestrian following},
	volume = {14},
	number = {2},
	urldate = {2016-06-01},
	journal = {Journal of Vision},
	author = {Rio, K. W. and Rhea, C. K. and Warren, W. H.},
	month = feb,
	year = {2014},
	pages = {4--4},

}

@incollection{fowler_language_2008,
	title = {Language {Use}, {Coordination}, and the {Emergence} of {Cooperative} {Action}},
	isbn = {978-3-540-74476-4 978-3-540-74479-5},
	abstract = {Over the last three decades, two major theoretical developments within cognitive science have enriched our understanding of perceptual function and coordinated action in real-world environments.},
	language = {en},
	urldate = {2013-12-24},
	booktitle = {Coordination: {Neural}, {Behavioral} and {Social} {Dynamics}},
	publisher = {Springer Berlin Heidelberg},
	author = {Fowler, Carol A. and Richardson, Michael J. and Marsh, Kerry L. and Shockley, Kevin D.},
	editor = {Fuchs, Armin and Jirsa, Viktor K.},
	month = jan,
	year = {2008},
	keywords = {Appl.Mathematics/Computational Methods of Engineering, Complexity, Dynamical Systems and Ergodic Theory, Human Physiology, Vibration, Dynamical Systems, Control},
	pages = {261--279},

}


@article{richardson_art_2007,
	title = {The art of conversation is coordination: common ground and the coupling of eye movements during dialogue},
	volume = {18},
	abstract = {When two people discuss something they can see in front of them, what is the relationship between their eye movements? We recorded the gaze of pairs of subjects engaged in live, spontaneous dialogue. Cross-recurrence analysis revealed a coupling between the eye movements of the two conversants. In the first study, we found their eye movements were coupled across several seconds. In the second, we found that this coupling increased if they both heard the same background information prior to their conversation. These results provide a direct quantification of joint attention during unscripted conversation and show that it is influenced by knowledge in the common ground.},
	language = {ENG},
	number = {5},
	journal = {Psychological Science},
	author = {Richardson, Daniel C. and Dale, Rick and Kirkham, Natasha Z.},
	month = may,
	year = {2007},
	pmid = {17576280},
	keywords = {attention, Comprehension, Eye movements, Fixation, Ocular, Humans, Reaction Time, Students, Time Factors, Verbal Behavior, Visual Perception},
	pages = {407--413}
}

@article{warren_dynamics_2006,
	title = {The {Dynamics} of {Perception} and {Action}},
	volume = {113},
	abstract = {L'article consiste en la présentation d'un framework pour la modélisation du comportement humain et animal. Le framework élaboré entre dans le cadre de l'approche écologique de l'action élaboré par Gibson (1979). Cette approche décrit le comportement humain sans utiliser de contrôleur centralisé ni de représentations internes.  Warren définit quatre thèmes centraux à cette approche : 
                  - l'agent est incarné et situé dans un environnement physique qui fournit des sources de contraintes
                  - le comportement est guidé par l'information disponible au sujet de l'état du système agent-environnement. L'information disponible est consideré comme une autre source de contrainte
                  - les relations de contrôle des actions sont spécifiques à la tâche, et font correspondre de l'information pertinente à des variables d'action pertinentes
                  - le comportement est émergent et auto-organisé, provenant de l'interaction entre l'agent et l'environnement, sous des contraintes physiques, informationnelles et de tâche. La dynamique de l'interaction rétroagit pour capturer les composants individuels, permettant de stabiliser des patterns d'action spécifiques
L'approche prise par Warren est de formaliser le comportement sous la forme de systèmes dynamiques. Le framework est décrit à deux niveaux : 
                - le cycle de perception-action comprend deux systèmes couplés : le système de l'environnement, décrivant son état du point de vue de l'agent, et la loi de contrôle, utilisée par l'agent pour agir dans l'environnement en fonction de l'information provenant de l'environnement
               - la dynamique comportementale décrivant le comportement sous forme d'une équation différentielle, utilisant des variables d'état ayant des dimensions pertinentes par rapport à la tâche. Les attracteurs dans l'espace des phases de la dynamique comportementale correspondent aux solutions stables du comportement, les répulseurs correspondent aux états à éviter et les bifurcations à des changements dans les patterns comportementaux. Le comportement est alors défini comme une trajectoire dans l'espace d'état de la dynamique comportementale.

L'agent situé dans le système doit régler la dynamique du système dans lequel il est embarqué pour chercher ses stabilités, il le fait par l'intermédiaire de la loi de contrôle. 
Lors de l'interaction agent-environnement, un champ de vecteur est crée au niveau de la dynamique comportementale avec l'apparition d'attracteurs. Le comportement se formant, rétroagit sur la loi de contrôle de l'agent de sorte à orienter ses actions vers le but.

Certains comportements sont néanmoins problématiques lorsque l'on cherche à les modéliser par une approche non symbolique : 
                        - comportement séquentiel : peut être décrit comme des séquences d'action définis par une série de sous-tâches et de sous-buts, ne pouvant pas être expliqué par les conditions environnementales. Une approche est d'utiliser une dynamique dont les variables d'états correspondent aux modes d'actions possibles. La trajectoire dans l'espace d'état de cette dynamique définirait l'enchainement des actions.
                        - comportement d'anticipation : les actions sont contraints non seulement par l'environnement immédiat mais aussi par un but distant. Une manière d'utiliser le comportement d'anticipation serait de définir une trajectoire à long terme qui émerge des interactions à court terme entre le système nerveux, le corps et l'environnement et est suffisant pour arriver au voisinage du but.
                        - comportement prédictif : une propriété cachée de l'objet contraint le comportement de l'agent.
                        - comportement stratégique : comprend les tâches dont le comportement est influencé par une connaissane contextuelle plus riche au sujet de l'aspect du monde avec lequel l'agent interagit.},
	number = {2},
	journal = {Psychological Review},
	author = {Warren, William H.},
	year = {2006},
	keywords = {lu},
	pages = {358--389},

}

@article{hjalmarsson_additive_2011,
	title = {The additive effect of turn-taking cues in human and synthetic voice},
	volume = {53},
	number = {1},
	journal = {Speech Communication},
	author = {Hjalmarsson, Anna},
	month = jan,
	year = {2011},
	keywords = {Conversational interfaces, Human-like interaction, speech synthesis, Turn-taking},
	pages = {23--35},

}


@article{kurtic_resources_2013,
	title = {Resources for turn competition in overlapping talk},
	volume = {55},
	abstract = {Overlapping talk occurs frequently in multi-party conversations, and is a domain in which speakers may pursue various communicative goals. The current study focuses on turn competition. Specifically, we seek to identify the phonetic differences that discriminate turn-competitive from non-competitive overlaps. Conversation analysis techniques were used to identify competitive and non-competitive overlaps in a corpus of multi-party recordings. We then generated a set of potentially predictive features relating to prosody (F0, intensity, speech rate, pausing) and overlap placement (overlap duration, point of overlap onset, recycling etc.). Decision tree classifiers were trained on the features and tested on a classification task, in order to determine which features and feature combinations best differentiate competitive overlaps from non-competitive overlaps. It was found that overlap placement features played a greater role than prosodic features in indicating turn competition. Among the prosodic features tested, F0 and intensity were the most effective predictors of turn competition. Also, our decision tree models suggest that turn competitive and non-competitive overlaps can be initiated by a new speaker at many different points in the current speaker’s turn. These findings have implications for the design of dialogue systems, and suggest novel hypotheses about how speakers deploy phonetic resources in everyday talk.},
	number = {5},
	urldate = {2014-02-14},
	journal = {Speech Communication},
	author = {Kurtić, Emina and Brown, Guy J. and Wells, Bill},
	month = jun,
	year = {2013},
	keywords = {Overlapping talk, Prosody, Turn competition, Turn end projection, Turn-taking},
	pages = {721--743},

}

@article{bogacz_physics_2006,
	title = {The physics of optimal decision making: a formal analysis of models of performance in two-alternative forced-choice tasks.},
	volume = {113},
	shorttitle = {The physics of optimal decision making},
	number = {4},
	journal = {Psychological review},
	author = {Bogacz, R. and Brown, E. and Moehlis, J. and Holmes, P. and Cohen, J.D.},
	year = {2006},
	pages = {700},

}

@article{lepora_embodied_2015,
	title = {Embodied {Choice}: {How} {Action} {Influences} {Perceptual} {Decision} {Making}},
	volume = {11},
	number = {4},
	journal = {PLOS Computational Biology},
	author = {Lepora, Nathan F. and Pezzulo, Giovanni},
	editor = {Daunizeau, Jean},
	month = apr,
	year = {2015},
	pages = {e1004110},

}

@article{kopp_architecture_2014,
	title = {An architecture for fluid real-time conversational agents: integrating incremental output generation and input processing},
	volume = {8},
	abstract = {Embodied conversational agents still do not achieve the fluidity and smoothness of natural conversational interaction. One main reason is that current system often respond with big latencies and in inflexible ways. We argue that to overcome these problems, real-time conversational agents need to be based on an underlying architecture that provides two essential features for fast and fluent behavior adaptation: a close bi-directional coordination between input processing and output generation, and incrementality of processing at both stages. We propose an architectural framework for conversational agents [Artificial Social Agent Platform (ASAP)] providing these two ingredients for fluid real-time conversation. The overall architectural concept is described, along with specific means of specifying incremental behavior in BML and technical implementations of different modules. We show how phenomena of fluid real-time conversation, like adapting to user feedback or smooth turn-keeping, can be realized with ASAP and we describe in detail an example real-time interaction with the implemented system.},
	language = {en},
	number = {1},
	urldate = {2014-08-19},
	journal = {Journal on Multimodal User Interfaces},
	author = {Kopp, Stefan and van Welbergen, Herwin and Yaghoubzadeh, Ramin and Buschmeier, Hendrik},
	month = mar,
	year = {2014},
	keywords = {ASAP, BMLA, Embodied conversational agents architecture, Fluid real-time interaction, Generation–interpretation coordination, Image Processing and Computer Vision, Incremental processing, Signal, Image and Speech Processing, User Interfaces and Human Computer Interaction},
	pages = {97--108},

}

@article{thorisson_mind_1999,
	title = {A {Mind} {Model} for {Multimodal} {Communicative} {Creatures} \& {Humanoids}},
	volume = {13},
	number = {4},
	journal = {International Journal of Applied Artificial Intelligence},
	author = {Th\'orisson, Kristinn R.},
	year = {1999},
	pages = {449--486},

}

@inproceedings{jegou_continuous_2015,
	title = {A {Continuous} {Model} for the {Management} of {Turn}-{Taking} in {User}-{Agent} {Spoken} {Interactions} {Based} on the {Variations} of {Prosodic} {Signals}},
	shorttitle = {Continuous model for {User}-{Agent} {Turn}-{Taking}},
	abstract = {Many recent works in agent architectures have focused on
polite and optimal turn-transitions. However, real turn-taking is more
complex due to several contextual variables, linked to each agents own
goals (cooperative or non cooperative for example). For engaging, mixedinitiative
interactions, we need thus to go beyond the polite agent context
to make emerge more complex patterns of turn-taking. We present here
an architecture based on a dynamical and continuous model of turntaking,
able to control the turn-taking behaviors of the agent depending
on its willingness to speak or not. We show how we implemented our
model based on human data and how complex patterns of turn-taking
emerge from agent-agent simulations. Finally we present the results of
a perceptual experiment where we questioned participants about the
intentions towards turn-taking of two agents interacting.},
	booktitle = {Intelligent {Virtual} {Agents} 2015},
	author = {J\'egou, Mathieu and Lefebvre, Liv and Chevaillier, Pierre},
	year = {2015},
	pages = {389--398}
}

@inproceedings{eyben_recent_2013,
	title = {Recent {Developments} in {openSMILE}, the {Munich} {Open}-source {Multimedia} {Feature} {Extractor}},
	abstract = {We present recent developments in the openSMILE feature extraction toolkit. Version 2.0 now unites feature extraction paradigms from speech, music, and general sound events with basic video features for multi-modal processing. Descriptors from audio and video can be processed jointly in a single framework allowing for time synchronization of parameters, on-line incremental processing as well as off-line and batch processing, and the extraction of statistical functionals (feature summaries), such as moments, peaks, regression parameters, etc. Postprocessing of the features includes statistical classifiers such as support vector machine models or file export for popular toolkits such as Weka or HTK. Available low-level descriptors include popular speech, music and video features including Mel-frequency and similar cepstral and spectral coefficients, Chroma, CENS, auditory model based loudness, voice quality, local binary pattern, color, and optical flow histograms. Besides, voice activity detection, pitch tracking and face detection are supported. openSMILE is implemented in C++, using standard open source libraries for on-line audio and video input. It is fast, runs on Unix and Windows platforms, and has a modular, component based architecture which makes extensions via plug-ins easy. openSMILE 2.0 is distributed under a research license and can be downloaded from http://opensmile.sourceforge.net/.},
	urldate = {2016-04-14},
	booktitle = {Proceedings of the 21st {ACM} {International} {Conference} on {Multimedia}},
	author = {Eyben, Florian and Weninger, Felix and Gross, Florian and Schuller, Björn},
	year = {2013},
	keywords = {acoustic features, affective computing, affect recognition, audio features, computational paralinguistics, feature extraction, machine learning, multimedia analysis, openSMILE, video features, visual features},
	pages = {835--838}
}

@inproceedings{baumann_inpro_2012,
	address = {Stroudsburg, PA, USA},
	title = {{INPRO\_iSS}: {A} {Component} for {Just}-in-time {Incremental} {Speech} {Synthesis}},
	shorttitle = {{INPRO}\_iSS},
	abstract = {We present a component for incremental speech synthesis (iSS) and a set of applications that demonstrate its capabilities. This component can be used to increase the responsivity and naturalness of spoken interactive systems. While iSS can show its full strength in systems that generate output incrementally, we also discuss how even otherwise unchanged systems may profit from its capabilities.},
	urldate = {2016-04-14},
	booktitle = {Proceedings of the {ACL} 2012 {System} {Demonstrations}},
	publisher = {Association for Computational Linguistics},
	author = {Baumann, Timo and Schlangen, David},
	year = {2012},
	pages = {103--108},

}

@inproceedings{skantze_towards_2010,
	title = {Towards incremental speech generation in dialogue systems},
	urldate = {2015-07-21},
	booktitle = {Proceedings of {SIGDIAL} 2010},
	publisher = {Association for Computational Linguistics},
	author = {Skantze, Gabriel and Hjalmarsson, Anna},
	year = {2010},
	pages = {1--8},

}

@inproceedings{de_vault_toward_2015,
	title = {Toward natural turn-taking in a virtual human negotiation agent},
	urldate = {2015-10-07},
	booktitle = {AAAI Spring Symposium on Turn-taking and Coordination in Human-Machine Interaction},
	address= {Stanford, CA.},
	author = {De Vault, David and Mell, Johnathan and Gratch, Jonathan},
	year = {2015},

}


@article{torreira_breathing_2015,
	title = {Breathing for answering: the time course of response planning in conversation},
	volume = {6},
	issn = {1664-1078},
	shorttitle = {Breathing for answering},
	urldate = {2015-10-07},
	journal = {Frontiers in Psychology},
	author = {Torreira, Francisco and B\"ogels, Sara and Levinson, Stephen C.},
	month = mar,
	year = {2015}

}

@article{skantze_turn-taking_2014,
	title = {Turn-taking, feedback and joint attention in situated human–robot interaction},
	volume = {65},
	issn = {0167-6393},
	doi = {10.1016/j.specom.2014.05.005},
	abstract = {In this paper, we present a study where a robot instructs a human on how to draw a route on a map. The human and robot are seated face-to-face with the map placed on the table between them. The user’s and the robot’s gaze can thus serve several simultaneous functions: as cues to joint attention, turn-taking, level of understanding and task progression. We have compared this face-to-face setting with a setting where the robot employs a random gaze behaviour, as well as a voice-only setting where the robot is hidden behind a paper board. In addition to this, we have also manipulated turn-taking cues such as completeness and filled pauses in the robot’s speech. By analysing the participants’ subjective rating, task completion, verbal responses, gaze behaviour, and drawing activity, we show that the users indeed benefit from the robot’s gaze when talking about landmarks, and that the robot’s verbal and gaze behaviour has a strong effect on the users’ turn-taking behaviour. We also present an analysis of the users’ gaze and lexical and prosodic realisation of feedback after the robot instructions, and show that these cues reveal whether the user has yet executed the previous instruction, as well as the user’s level of uncertainty.},
	urldate = {2014-09-24},
	journal = {Speech Communication},
	author = {Skantze, Gabriel and Hjalmarsson, Anna and Oertel, Catharine},
	month = nov,
	year = {2014},
	keywords = {Feedback, Gaze, Joint attention, Prosody, Turn-taking, Uncertainty},
	pages = {50--66},

}

@incollection{al_moubayed_regulating_2015,
	title = {Regulating {Turn}-{Taking} in {Multi}-child {Spoken} {Interaction}},
	isbn = {978-3-319-21995-0 978-3-319-21996-7},
	abstract = {We examine the effects of coordinated head and eye movement on children’s turn-taking behavior in the context of a multiparty game. Twenty-two pairs of children competed in a trivia quiz scenario that is moderated first by a human and later by a robot. We quantify the effects of eyes-only and combined head-eye movements on the turn-taking behavior of the children in both directed and open questions (where either child is free to respond to win the point), and compare the results to performance with the human moderator who uses natural head and eye movements as well as additional cues that can be relevant to turn-taking. We find that coordinated head and eye movement in the robot is a significantly more successful cueing strategy than eye movement alone in directed questions, producing turn regulation that is comparable to the human moderator’s more complex behaviors. Further, in open questions, head gaze results in more balanced turn-taking than eye movement alone. Finally, we compare the results for children to comparable studies with adults and discuss the implications for developing computational models of joint-attention in human-agent spoken interactions.},
	booktitle = {Intelligent {Virtual} {Agents}},
	publisher = {Springer International Publishing},
	author = {Al Moubayed, Samer and Lehman, Jill},
	editor = {Brinkman, Willem-Paul and Broekens, Joost and Heylen, Dirk},
	month = aug,
	year = {2015},
	keywords = {Artificial Intelligence (incl. Robotics), Behavior synthesis, Child-robot interaction, Computers and Society, Head eyes coordination, health informatics, Information Storage and Retrieval, Information Systems Applications (incl. Internet), multiparty interaction, Spoken dialog systems, Turn-taking, User Interfaces and Human Computer Interaction},
	pages = {363--374}
}

@inproceedings{mutlu_storytelling_2006,
	title = {A storytelling robot: {Modeling} and evaluation of human-like gaze behavior},
	shorttitle = {A storytelling robot},
	urldate = {2017-02-02},
	booktitle = {Humanoid robots, 2006 6th {IEEE}-{RAS} international conference on},
	publisher = {IEEE},
	author = {Mutlu, Bilge and Forlizzi, Jodi and Hodgins, Jessica},
	year = {2006},
	pages = {518--523}
}

@inproceedings{oertel_gaze_2013,
	title = {Gaze patterns in turn-taking},
	language = {eng},
	urldate = {2017-04-08},
	booktitle = {13th {Annual} {Conference} of the {International} {Speech} {Communication} {Association} 2012 ({INTERSPEECH} 2012)},
	author = {Oertel, Catharine and Wlodarczak, Marcin and Edlund, Jens and Wagner, Petra and Gustafson, Joakim},
	year = {2013},
}

@inproceedings{bohus_facilitating_2010,
	address = {New York, NY, USA},
	series = {{ICMI}-{MLMI} '10},
	title = {Facilitating {Multiparty} {Dialog} with {Gaze}, {Gesture}, and {Speech}},
	isbn = {978-1-4503-0414-6},
	abstract = {We study how synchronized gaze, gesture and speech rendered by an embodied conversational agent can influence the flow of conversations in multiparty settings. We begin by reviewing a computational framework for turn-taking that provides the foundation for tracking and communicating intentions to hold, release, or take control of the conversational floor. We then present implementation aspects of this model in an embodied conversational agent. Empirical results with this model in a shared task setting indicate that the various verbal and non-verbal cues used by the avatar can effectively shape the multiparty conversational dynamics. In addition, we identify and discuss several context variables which impact the turn allocation process.},
	urldate = {2017-04-10},
	booktitle = {International {Conference} on {Multimodal} {Interfaces} and the {Workshop} on {Machine} {Learning} for {Multimodal} {Interaction}},
	publisher = {ACM},
	author = {Bohus, Dan and Horvitz, Eric},
	year = {2010},
	keywords = {behavioral models, floor management, Gaze, Gesture, multimodal systems, multiparty interaction, multiparty turn-taking, situated interaction, Speech, spoken dialog},
	pages = {1--8}
}

@article{ratcliff_note_1980,
	title = {A note on modeling accumulation of information when the rate of accumulation changes over time},
	volume = {21},
	number = {2},
	journal = {Journal of Mathematical Psychology},
	author = {Ratcliff, Roger},
	month = apr,
	year = {1980},
	pages = {178--184}
}

@inproceedings{bevacqua_multimodal_2010,
	title = {Multimodal backchannels for embodied conversational agents},
	author = {Bevacqua, Elisabetta and Pammi, Sathish and Hyniewska, Sylwia Julia and Schröder, Marc and Pelachaud, Catherine},
	bookTitle="Intelligent Virtual Agents: 10th International Conference, IVA 2010, Philadelphia, PA, USA, September 20-22, 2010. Proceedings",
	year = {2010},
	pages = {194--200}
}

@inproceedings{buschmeier_when_2014,
	title = {When to elicit feedback in dialogue: {Towards} a model based on the information needs of speakers},
	shorttitle = {When to elicit feedback in dialogue},
	abstract = {Communicative feedback in dialogue is an important mechanism that helps interlocutors coordinate their interaction. Listeners pro-actively provide feedback when they think that it is important for the speaker to know their mental state, and speakers pro-actively seek listener feedback when they need information on whether a listener perceived, understood or accepted their message. This paper presents first steps towards a model for enabling attentive speaker agents to determine when to elicit feedback based on continuous assessment of their information needs about a user's listening state.},
	language = {eng},
	booktitle = {Proceedings of the 14th {International} {Conference} on {Intelligent} {Virtual} {Agents}},
	author = {Buschmeier, Hendrik and Kopp, Stefan},
	year = {2014}
}

@inproceedings{paek_continuous_2000,
	title = {Continuous listening for unconstrained spoken dialog.},
	urldate = {2017-04-18},
	booktitle = {{INTERSPEECH}},
	author = {Paek, Tim and Horvitz, Eric and Ringger, Eric K.},
	year = {2000},
	pages = {138--141}
}

@article{kelso_coordination_2009,
	title = {Coordination dynamics},
	abstract = {L'article consiste en une description de la dynamique de la coordination proposé par Kelso. La dynamique de la coordination, décrit comment des patterns de coordination sont crées dans des systèmes dynamiques, auto-organisés et vivant. La coordination d'un système biologique est vue comme une propriété émergente des interactions entre les individus d'un système. Le pattern capture ensuite le comportement des individus du système. Un système peut être vu à trois niveaux : les conditions limites d'apparition d'un pattern, le niveau collectif et sa dynamique, le niveau composant incluant la propre dynamique des composant et leur couplage non-linéaire avec leurs voisins. Le système peut exhiber différents patterns de coordination, leur condition d'apparition étant déterminé par les paramètres de contrôle. Lorsqu'un paramètre de contrôle atteint un certain seuil, il peut provoquer un changement de pattern, ce phénomène est appelé transition de phase. Les interaction entre les individus, créent de même des variables d'état du comportement global, ces variables sont appelées les variables collectives. La dynamique de la coordination a permis l'étude d'un certain nombre de phénomènes de synchronisation notamment la transition entre un pattern "anti-phase" à un pattern "phase" dans différentes tâche de synchronisation (mouvement de deux doigt d'une même personne, mouvement de deux jambes de deux personnes, synchronisation d'une action sur un signal sonore). L'étude de ces phénomènes a permis de mettre en évidence la notion de métastabilité, un état où le système ne comporte plus d'attracteur, mais où il reste suffisamment proche de la bifurcation pour avoir une attraction envers l'ancien attracteur. Plus précisément, chaque individu du système exhibe deux tendances contradictoires : la tendance à se comporter selon le collectif et la tendance à se comporter individuellement.},
	journal = {Encyclopedia of complexity and systems sciences},
	author = {Kelso, J.A.S},
	year = {2009},
	pages = {1537--1564}
}

@article{ratcliff_theory_1978,
	title = {A theory of memory retrieval},
	volume = {85},
	number = {2},
	journal = {Psychological Review},
	author = {Ratcliff, Roger},
	year = {1978},
	pages = {59--108}
}

@inproceedings{cavazza_towards_2014,
	title = {Towards empathic neurofeedback for interactive storytelling},
	volume = {41},
	urldate = {2017-05-26},
	booktitle = {{OASIcs}-{OpenAccess} {Series} in {Informatics}},
	publisher = {Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik},
	author = {Cavazza, Marc and Aranyi, Gabor and Charles, Fred and Porteous, Julie and Gilroy, Stephen and Klovatch, Ilana and Jackont, Gilan and Soreq, Eyal and Keynan, Nimrod Jakob and Cohen, Avihay and {others}},
	year = {2014}
}

@article{lucas_its_2014,
	title = {It’s only a computer: {Virtual} humans increase willingness to disclose},
	volume = {37},
	issn = {0747-5632},
	shorttitle = {It’s only a computer},
	abstract = {Research has begun to explore the use of virtual humans (VHs) in clinical interviews (Bickmore, Gruber, \&amp; Picard, 2005). When designed as supportive and “safe” interaction partners, VHs may improve such screenings by increasing willingness to disclose information (Gratch, Wang, Gerten, \&amp; Fast, 2007). In health and mental health contexts, patients are often reluctant to respond honestly. In the context of health-screening interviews, we report a study in which participants interacted with a VH interviewer and were led to believe that the VH was controlled by either humans or automation. As predicted, compared to those who believed they were interacting with a human operator, participants who believed they were interacting with a computer reported lower fear of self-disclosure, lower impression management, displayed their sadness more intensely, and were rated by observers as more willing to disclose. These results suggest that automated VHs can help overcome a significant barrier to obtaining truthful patient information.},
	urldate = {2014-08-18},
	journal = {Computers in Human Behavior},
	author = {Lucas, Gale M. and Gratch, Jonathan and King, Aisha and Morency, Louis-Philippe},
	month = aug,
	year = {2014},
	keywords = {Clinical interviews, Computer-assisted assessment, Honest responding, Self-disclosure, virtual humans},
	pages = {94--100}
}
