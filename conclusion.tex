\section{Conclusion}

\subsection{Contribution of this article}

We presented three different contributions in this article. First, we elaborated a theoretical model for the coordination of turn between the user and the agent. Our assumption is that the coordination of turns is partly an emergent property that comes from the sensorimotor coupling between the participants. 
%We simulated the model in the context of theoretical agent-agent simulations. 
%We showed in this context the capacity of our agents to coordinate their speaking turns and we reproduces qualitatively the situations observed in human interactions. 
The agents controlled by our theoretical model are adaptative, they are able to ensure a coordination in different types of environment and in interactions with partners that have more or less inertia in producing their signals or perceive more or less rapidly the behavior of their partner. Even if we presented only two types of adaptation, to the absence of presence of signals and to the way the user produces its signals, these simulations show the capability of an emergent model to reproduce the situations linked to the coordination of turns observed in human conversations.

This adaptatibility is essential as there exists a variety of ways users coordinate their turns with the agent. The adaptation is based on the idea that the participants modify continuously their verbal and nonverbal productions in order to keep the coordination effective. It highlights the fact that, not only the agent must be able to perceive reliably the user's intentions, but the user has also to perceive reliably the agent's behavior in different environmnental settings. One way to guaranty that the user still perceive reliably the agent's behavior is that the agent adapts its behavior to clarify its intentions to the user, capacity that we observed in our simulations without needing to modify the equations controling the signal productions. This view of user-agent adaptation is new in the context turn-taking, as past approaches relied on offline machine learning techniques or online reinforcement learning to perceive reliably the behavior of the user, without taking into account the way the user perceives the agent's behavior. 

However, this principle of adaptation requires also the ability for the user to adapt his signals production in order to clarify his behavior and intentions. Whether the user could adapt his behavior to the agent and environment is currently a research questions, and needs further analyses of user-agent interactions. 
Some clues let us think that a kind of adaptation of the user to the agent is done, as we observed a variability in the silence durations after the end of the agent's turn and before the user's beginning of turn, whether the user interacted with the agent controlled by our model or with an agent controlled by a model based on temporal thresholds. Nevertheless, we do not know the exact reason of this variability, it could be due to the prosodic signals produced by the agent at the end of its turn or linked to a temporal alignment of the user to the way the agent takes the turn, a phenomenon that is observed in conversations. 

As a second contribution, we propose an agent architecture, named BeAware, for the implementation of both continuous and discrete control models of behavior. 
Indeed very few embodied conversational agents can combine these two types of model. 
%Our model distinguish from traditional evenemential, purely reactive models and rather is based on a continuous perception of the user's behavioral cues and a continuous modulation of the agent's actions. Few actual embodied conversational agents permit to implement these types of models. 
%In order to integrate our model in a embodied conversational agent architecture 
Our solution is an extension of the ASAP architecture \cite{kopp_architecture_2014}, enriched with principles inspired from Ymir \citep{thorisson_mind_1999}. 
The solution complies with the SAIBA standard and thus, our model can be integrated with existing modules that manage other aspects of multimodal user-agent interactions, such as expressive verbal prodcution, emotions or interpersonal attitudes. These abilies require to implement realizers for managing concurrently, among others, the prosody, the gestures, head movements, or the gaze direction of the agent.
%By extending ASAP compliant with the current SAIBA standard, we made our model integrable with existing modules, managing other aspects of user-agent interactions, such as verbal interaction, emotions or interpersonal attitudes or realizers managing gestures or the gaze direction of the agent. As a demonstration, we showed the capability of an agent controlled by our model to manage turns in real-time dialogical interactions with users, and showed that in some ways, our model work with real-time user-agent behaviors. 


%Finally, we analyzed real-time user-agent interactions. 
%The aim was to complement existing perceptual studies about the user's experience with an agent modulating its turns, and verifying the effectiveness of the coordination between several interactions with different users. 
Finally, we analyzed how the agent's turn-taking strategy may impact the user's experience and the effectiveness of the coordination.
We measured factors related to the credibility of the agent, its social presence, the user's satisfaction and the easiness of the interaction.  
%We did not observe significant differences in the answers of the participants to the questions about those factors. 
Undoubtedly, the quality of the coordination is multifactorial and requires to finely perceive and modulates a rich set of verbal and nonverbal signals. It also requires effective incremental speech recognition and generation. None of the existing components supporting these processes have reach nowadays the desired quality to support natural interactions. Our experiment showed that users did not always interpreted the agent's turn-taking as voluntary but as implementation artifacts and thus experienced difficulties to coordinate with the agent. 
However, our experimental setup has some limitations. 
First the coordination was only based on the modulation of the prosody. Adding other perceptual channels should enriched the information available for the coordination and thus making it more effective.
Second, we focused on the most critical situation, namely long overlaps of speaking turns. In these conditions, the agent should have be able to modify its verbal contents when barging-in with the users (e.g. word recycling, use of fillers, etc). This inconsistency may explain the way participants perceived the agents behavior. 
Finally we set manually the parameters of control equations. Optimizing these parameters could result in a better coordination. 
%We also found that participants did not always perceived the interruption of the agent as voluntary, and we found that the agent had troubles sometimes coordinating with the user. 
%Different elements can be improved to improve the interaction between the user and the agent. 
%First, the unvolontary and voluntary aspect of the interruptions are likely to be linked with the verbal content of the agent's utterance, and maybe there need to be a particular way to formulate the utterance when interrupting the user. It would then necessitates an extension of our actual model, linked to the management of the verbal content linked to turn-taking. 
%Second, the agent only interacts with the user by interpreting and modulating prosody, adding multimodal interpretation abilities could certainly improve the way the agent coordinates with the user. 
%Also for this implementation, since we determined manually the parameters of the agent's equations, optimizing these parameters could improve the coordination between the user and the agent. 

\subsection{Future work}

%As mentioned in Section \ref{backgd}, a complete turn-taking model relies on different cognitive processes, from sensorimotor coupling to projection.
%, and in order to have a complete turn-taking model, we need to take into account all these processes. 
%One of the major question regarding these processes in how verbal content interpretation should be done by an agent. To address this question, \cite{de_vault_incremental_2011} proposed a statistical model where the next sentence and the content is predicted by the agent. This method could work in real-time interactions when the domain of discourse is well determined but as the dialog capacities and the set of utterance the user could say increases this method could become rapdily fastidious or even impossible to perform by an agent. 
As mentionned in section \ref{backgd}, verbal content is a main source of information for the listener to detect ends of turn \cite{de_ruiter_projecting_2006}, and should considerably help participants to coordinate their speaking turns. Nevertheless, the way we could implement the interpretation mechanism in an embodied conversational agents remains unclear, firstly because the process used by human participants to coordinate turns based on verbal informations is still debated \citep{heldner_pauses_2010,magyari_prediction_2012,riest_anticipation_2015}. Secondly, because the way hypothesized by a majority of authors implies perceiving reliably word by word the utterance said by the user and be able to perceive and reason about the syntax of the utterance in real-time \citep{sacks_simplest_1974}, capacities that current natural language understanding components barely have currently  \cite{de_vault_incremental_2011}.


% Plan : 
%	- Nous considérons que la motivation à changer de rôle varie selon un ensemble de facteurs liés à l'interaction entre les deux agents
%	- dans cet article nous ne précisons pas la manière dont la motivation à changer de rôle varie selon ces facteurs
%	- Avoir un turn-taking plus réaliste : comment différents facteurs contextuels influent sur la motivation à changer de rôle
%	- Auteurs ont exploré comment les attitudes interpersonnelles où l'importance de l'énoncé à prononcer par l'agent pouvait modifier le comportement de turn-taking
%	- Néanmoins, aucun modèle n'a traité de la manière dont les émotions de l'agent pouvaient impacter le comportement de turn-taking
%	- Nous envisageons d'explorer la manière dont la motivation à changer de rôle varie selon le comportement de turn-taking de l'agent
Moreover, in all interaction scenarios presented in this article,  we determined manually the agent's motivation to change role that remained constant throughout the simulations. In order to have a more realistic turn-taking, motivation to change role should vary according to several factors such as interpersonal attitudes, importance of the agent's verbal contribution to dialog, personality or emotions. While past computational models linked interpersonal attitudes \citep{ravenet_conversational_2015} or importance of the verbal contribution \citep{selfridge_bidding_2009} with turn-taking behaviors, no existing model explored the links between the agent's emotional state and its turn-taking behavior. We plan to treat this question by adding a module that compute motivation to change role according to the agent's emotions. 

%Moreover, our model makes intervene a motivation to change role and a motivation to keep role. 
Finally, in human conversation, when participants are motivated to be and stay listeners, they produce backchannels to inform the current speaker they still listen what he is saying and even encourage him to go on speaking. 
We could exploit our motivation variable to trigger and drive some behaviors related to grounding such as backchannels. 
For the speaker, the detection of backchannels could impact its perceptual decision-making process, as a user producing backchannels likely does not take the turn immediately.
We also plan to explore in more details the mechanism of alignment linked to the coordination of turns, as highlighted by \citep{benus_pragmatic_2011}.
We guess that our model could account for the temporal alignment observed in human conversations.  

%Other extension possibility is linked to the management of speaking turns in multiparty interactions, which raises issues like addressing one or a subset of participants and for the listener to define its role in conversations (addressee, bystander, overhearer), managing conflicts between several listeners trying to take the turn at the same time. These issues are linked to a more complex management of decision-making process and to control the agent's relative location to other participants.
%gaze direction, the management of the participants locations. 



% 